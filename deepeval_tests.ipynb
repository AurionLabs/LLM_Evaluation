{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidated DeepEval Test Execution\n",
    "\n",
    "This notebook aggregates the DeepEval-based evaluation scenarios defined in the repository's Python test files. Each metric-oriented suite is recreated with a lightweight dataset so that the evaluations can be orchestrated from a single location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from deepeval.metrics import (\n",
    "    FaithfulnessMetric,\n",
    "    HallucinationMetric,\n",
    "    PromptAlignmentMetric,\n",
    "    SummarizationMetric,\n",
    "    JsonCorrectnessMetric,\n",
    "    AnswerRelevancyMetric,\n",
    "    GEval,\n",
    ")\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleSchema(BaseModel):\n",
    "    name: str\n",
    "\n",
    "def as_markdown_table(rows, headers):\n",
    "    lines = [\n",
    "        '|' + '|'.join(headers) + '|',\n",
    "        '|' + '|'.join(['---'] * len(headers)) + '|',\n",
    "    ]\n",
    "    for row in rows:\n",
    "        values = [str(row.get(h, '') if row.get(h, '') is not None else '') for h in headers]\n",
    "        lines.append('|' + '|'.join(values) + '|')\n",
    "    return '\n'.join(lines)\n",
    "\n",
    "dataset = [\n",
    "    {\n",
    "        'test_suite': 'Faithfulness',\n",
    "        'metric_name': 'FaithfulnessMetric',\n",
    "        'metric_factory': lambda: FaithfulnessMetric(threshold=0.7, model='gpt-4o-mini', include_reason=True),\n",
    "        'cases': [\n",
    "            {\n",
    "                'case_id': 'faithfulness_case_1',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"What if these shoes don't fit?\",\n",
    "                    'actual_output': \"We offer a 30-day full refund at no extra cost.\",\n",
    "                    'retrieval_context': [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'test_suite': 'Hallucination',\n",
    "        'metric_name': 'HallucinationMetric',\n",
    "        'metric_factory': lambda: HallucinationMetric(threshold=0.5, model='gpt-4o-mini', include_reason=True),\n",
    "        'cases': [\n",
    "            {\n",
    "                'case_id': 'hallucination_case_1',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"What was the blond doing?\",\n",
    "                    'actual_output': \"A blond drinking water in public.\",\n",
    "                    'context': [\"A man with blond-hair, and a brown shirt drinking out of a public water fountain.\"]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'test_suite': 'Prompt Alignment',\n",
    "        'metric_name': 'PromptAlignmentMetric',\n",
    "        'metric_factory': lambda: PromptAlignmentMetric(prompt_instructions=[\"Reply in all uppercase\"], model='gpt-4o-mini', include_reason=True),\n",
    "        'cases': [\n",
    "            {\n",
    "                'case_id': 'prompt_alignment_case_1',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"What is capital of India?\",\n",
    "                    'actual_output': \"THE CAPITAL OF INDIA IS NEW DELHI.\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'test_suite': 'Summarization',\n",
    "        'metric_name': 'SummarizationMetric',\n",
    "        'metric_factory': lambda: SummarizationMetric(threshold=0.7, model='gpt-4o-mini'),\n",
    "        'cases': [\n",
    "            {\n",
    "                'case_id': 'summarization_case_1',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"Rice is the staple food of Bengal. Bhortas (lit-\\\"mashed\\\") are a really common type of food used as an additive too rice. there are several types of Bhortas such as Ilish bhorta shutki bhorta, begoon bhorta and more. Fish and other seafood are also important because Bengal is a reverrine region.\\nSome fishes like puti (Puntius species) are fermented. Fish curry is prepared with fish alone or in combination with vegetables. Shutki maach is made using the age-old method of preservation where the food item is dried in the sun and air, thus removing the water content. This allows for preservation that can make the fish last for months, even years in Bangladesh\",\n",
    "                    'actual_output': \"Bengali cuisine centers on rice and diverse mashed accompaniments called bhortas, along with plentiful fish dishes that are often dried or fermented to extend their shelf life.\"\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'test_suite': 'GEval Correctness',\n",
    "        'metric_name': 'GEval',\n",
    "        'metric_factory': lambda: GEval(\n",
    "            name='Correctness',\n",
    "            model='gpt-4o-mini',\n",
    "            evaluation_params=[\n",
    "                LLMTestCaseParams.EXPECTED_OUTPUT,\n",
    "                LLMTestCaseParams.ACTUAL_OUTPUT,\n",
    "            ],\n",
    "            evaluation_steps=[\n",
    "                \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "                \"Lightly penalize omission of detail while focusing on the main idea\",\n",
    "                \"Vague language, or contradicting opinions, are acceptable\",\n",
    "            ],\n",
    "        ),\n",
    "        'cases': [\n",
    "            {\n",
    "                'case_id': 'geval_case_1',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"What are the main causes of deforestation?\",\n",
    "                    'actual_output': \"The main causes of deforestation include agricultural expansion, logging, infrastructure development, and urbanization.\",\n",
    "                    'expected_output': \"The main causes of deforestation include agricultural expansion, logging, infrastructure development, and urbanization.\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'case_id': 'geval_case_2',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"Define the term 'artificial intelligence'.\",\n",
    "                    'actual_output': \"Artificial intelligence is the simulation of human intelligence by machines.\",\n",
    "                    'expected_output': \"Artificial intelligence refers to the simulation of human intelligence in machines that are programmed to think and learn like humans, including tasks such as problem-solving, decision-making, and language understanding.\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'case_id': 'geval_case_3',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"List the primary colors.\",\n",
    "                    'actual_output': \"The primary colors are green, orange, and purple.\",\n",
    "                    'expected_output': \"The primary colors are red, blue, and yellow.\"\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'test_suite': 'JSON Correctness',\n",
    "        'metric_name': 'JsonCorrectnessMetric',\n",
    "        'metric_factory': lambda: JsonCorrectnessMetric(expected_schema=ExampleSchema, model='gpt-4o-mini', include_reason=True),\n",
    "        'cases': [\n",
    "            {\n",
    "                'case_id': 'json_correctness_case_1',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"Output me a random Json with the 'name' key\",\n",
    "                    'actual_output': '{\\\"name\\\": \\\"A Random Name\\\"}'\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'test_suite': 'Answer Relevancy',\n",
    "        'metric_name': 'AnswerRelevancyMetric',\n",
    "        'metric_factory': lambda: AnswerRelevancyMetric(threshold=0.7, model='gpt-4o-mini'),\n",
    "        'cases': [\n",
    "            {\n",
    "                'case_id': 'relevancy_case_1',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"Can I return these shoes after 30 days?\",\n",
    "                    'actual_output': \"Yes, you can return them. We offer a 30-day full refund. Do you have your original receipt?\",\n",
    "                    'retrieval_context': [\n",
    "                        \"All customers are eligible for a 30-day full refund at no extra cost.\",\n",
    "                        \"Returns are only accepted within 30 days of purchase.\",\n",
    "                    ],\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'case_id': 'relevancy_case_2',\n",
    "                'llm_test_case_kwargs': {\n",
    "                    'input': \"Can I return these shoes after 30 days?\",\n",
    "                    'actual_output': \"Unfortunately, returns are only accepted within 30 days of purchase.\",\n",
    "                    'retrieval_context': [\n",
    "                        \"All customers are eligible for a 30-day full refund at no extra cost.\",\n",
    "                        \"Returns are only accepted within 30 days of purchase.\",\n",
    "                    ],\n",
    "                }\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The structure below mirrors the original test scenarios so that each metric can be executed programmatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_rows = []\n",
    "for entry in dataset:\n",
    "    for case in entry['cases']:\n",
    "        row = {\n",
    "            'test_suite': entry['test_suite'],\n",
    "            'metric': entry['metric_name'],\n",
    "            'case_id': case['case_id'],\n",
    "            'input': case['llm_test_case_kwargs'].get('input'),\n",
    "            'actual_output': case['llm_test_case_kwargs'].get('actual_output'),\n",
    "            'additional_args': json.dumps({k: v for k, v in case['llm_test_case_kwargs'].items() if k not in ['input', 'actual_output']})\n",
    "        }\n",
    "        dataset_rows.append(row)\n",
    "\n",
    "headers = ['test_suite', 'metric', 'case_id', 'input', 'actual_output', 'additional_args']\n",
    "table_md = as_markdown_table(dataset_rows, headers)\n",
    "display(Markdown(table_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Execution\n",
    "\n",
    "Each case is evaluated using its corresponding metric. The resulting scores, reasoning (when provided by the metric), and execution status are consolidated below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for entry in dataset:\n",
    "    for case in entry['cases']:\n",
    "        metric = entry['metric_factory']()\n",
    "        test_case = LLMTestCase(**case['llm_test_case_kwargs'])\n",
    "        try:\n",
    "            metric.measure(test_case)\n",
    "            score = getattr(metric, 'score', None)\n",
    "            reason = getattr(metric, 'reason', '')\n",
    "            status = 'success'\n",
    "        except Exception as exc:\n",
    "            score = None\n",
    "            reason = str(exc)\n",
    "            status = 'error'\n",
    "        results.append({\n",
    "            'test_suite': entry['test_suite'],\n",
    "            'metric': entry['metric_name'],\n",
    "            'case_id': case['case_id'],\n",
    "            'score': score,\n",
    "            'reason_or_error': reason,\n",
    "            'status': status,\n",
    "        })\n",
    "\n",
    "result_headers = ['test_suite', 'metric', 'case_id', 'score', 'reason_or_error', 'status']\n",
    "results_table_md = as_markdown_table(results, result_headers)\n",
    "display(Markdown(results_table_md))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
